import time
import requests
import os
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from . import login
from pathlib import Path

CURRENT_SEMESTER_LABEL = os.getenv("CURRENT_SEMESTER_LABEL", "25-26å­¦å¹´ç¬¬1å­¦æœŸ") # æ‰‹åŠ¨æŒ‡å®šâ€œæœ¬å­¦æœŸâ€çš„æ ‡è®°
SEMESTER_PATTERN = re.compile(r"\d{2}-\d{2}å­¦å¹´ç¬¬[1-2]å­¦æœŸ") # ç”¨æ¥ä»è¯¾ç¨‹åé‡Œæå– â€œ25-26å­¦å¹´ç¬¬1å­¦æœŸâ€ è¿™ä¸€æ®µ


DOWNLOAD_DIR = 'downloads'
if not os.path.exists(DOWNLOAD_DIR):
    os.makedirs(DOWNLOAD_DIR)

def get_filename_from_response(response, fallback_url):
    
    """ä»å“åº”å¤´æˆ–URLæå–çœŸå®æ–‡ä»¶å"""
    cd = response.headers.get('Content-Disposition', '')
    filename = None

    if 'filename=' in cd:
        import re
        match = re.search(r'filename\*?=(?:UTF-8\'\')?"?([^";]+)"?', cd)
        if match:
            filename = match.group(1)
            filename = requests.utils.unquote(filename)  # è§£ç ä¸­æ–‡å

    if not filename:
        filename = os.path.basename(urlparse(fallback_url).path)

    # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•æ ¹æ® Content-Type æ¨æ–­
    if not os.path.splitext(filename)[1]:
        content_type = response.headers.get("Content-Type", "").lower()
        if "pdf" in content_type:
            filename += ".pdf"
        elif "word" in content_type:
            filename += ".docx"
        elif "powerpoint" in content_type:
            filename += ".pptx"
        else:
            filename += ".bin"

    return filename


def ensure_unique_filename(filename):
    """è‹¥é‡ååˆ™è‡ªåŠ¨åŠ ç¼–å·"""
    base, ext = os.path.splitext(filename)
    counter = 1
    while os.path.exists(os.path.join(DOWNLOAD_DIR, filename)):
        filename = f"{base}_{counter}{ext}"
        counter += 1
    return filename


def download_file(file_url, session):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ° downloads æ–‡ä»¶å¤¹"""
    try:
        response = session.get(file_url, stream=True)
        response.raise_for_status()

        filename = get_filename_from_response(response, file_url)
        filename = ensure_unique_filename(filename)
        save_path = os.path.join(DOWNLOAD_DIR, filename)

        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {save_path}")
        return save_path
    except Exception as e:
        print(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {file_url}: {e}")
        return None


def get_all_courses(session):
    """
    ä»è¯¾ç¨‹é¦–é¡µè§£æå½“å‰è´¦å·çš„æ‰€æœ‰è¯¾ç¨‹å…¥å£ã€‚

    è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ï¼š
      {
        "name": "è¯¾ç¨‹æ˜¾ç¤ºå",
        "course_id": "_83408_1",
        "launcher_url": "https://course.pku.edu.cn/webapps/blackboard/execute/launcher?type=Course&id=PkId{key=_83408_1,...}&url="
      }
    """
    # å°è¯•ä¸¤ä¸ªå…¥å£ï¼šç™»å½•é¦–é¡µå’ŒåŸæ¥çš„ portal tab é¡µé¢
    candidate_urls = [
        login.COURSE_BASE_URL,  # ä¸€èˆ¬æ˜¯ https://course.pku.edu.cn/
        "https://course.pku.edu.cn/webapps/portal/execute/tabs/tabAction?tab_tab_group_id=_1_1",
    ]

    html = None
    base_url = None

    for url in candidate_urls:
        try:
            resp = session.get(url, allow_redirects=True)
            resp.raise_for_status()
            # ç²—ç•¥åˆ¤æ–­ä¸€ä¸‹é¡µé¢é‡Œæ˜¯ä¸æ˜¯æœ‰ "type=Course" è¿™æ ·çš„è¯¾ç¨‹å…¥å£
            if "execute/launcher" in resp.text and "type=Course" in resp.text:
                html = resp.text
                base_url = resp.url  # æ³¨æ„å¯èƒ½é‡å®šå‘
                print(f"åœ¨é¡µé¢ {resp.url} æ‰¾åˆ°è¯¾ç¨‹åˆ—è¡¨ã€‚")
                break
        except requests.exceptions.RequestException as e:
            print(f"è®¿é—® {url} å¤±è´¥: {e}")
            continue

    if not html:
        print("æœªåœ¨è¯¾ç¨‹é¦–é¡µå‘ç°è¯¾ç¨‹åˆ—è¡¨ HTMLã€‚")
        return []

    soup = BeautifulSoup(html, "html.parser")
    courses = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        text = a.get_text(strip=True)
        # åŒ¹é…ï¼šè¯¾ç¨‹å…¥å£é“¾æ¥
        #   /webapps/blackboard/execute/launcher?type=Course&id=PkId{key=_83408_1,...}&url=
        if "execute/launcher" in href and "type=Course" in href and "PkId{key=" in href:
            full_url = urljoin(base_url, href)

            # è¡¥å……ä¸€æ®µï¼šæ£€æŸ¥æ˜¯ä¸æ˜¯æœ¬å­¦æœŸçš„è¯¾ç¨‹ï¼Œåªé€‰æ‹©æœ¬å­¦æœŸçš„ï¼›å¦‚æœæƒ³è¦æ‰€æœ‰å­¦æœŸçš„åˆ™åˆ æ‰è¿™æ®µ
            sem_match = SEMESTER_PATTERN.search(text)
            semester_label = sem_match.group(0) if sem_match else None
            if semester_label is not None and semester_label != CURRENT_SEMESTER_LABEL: 
                continue # è¿™æ˜¯å†å²å­¦æœŸ â†’ ç›´æ¥ continue

            # ä» id å‚æ•°ä¸­æå– course_id: PkId{key=_83408_1, ...}
            m = re.search(r"PkId\{key=([^,}]+)", href)
            course_id = m.group(1) if m else None

            courses.append({
                "name": text,
                "course_id": course_id,
                "launcher_url": full_url,
            })

    print(f"è¯¶ï¼ è§£æåˆ° {len(courses)} é—¨è¯¾ç¨‹ã€‚")
    for c in courses:
        print(f"  - {c['course_id']}: {c['name']}")

    return courses


def get_course_content_pages(session, course):
    """
    ç»™å®šä¸€é—¨è¯¾ç¨‹ï¼ˆåŒ…å« launcher_url, course_idï¼‰ï¼Œ
    è®¿é—®è¯¾ç¨‹ä¸»é¡µï¼Œä»ä¸­è§£æå‡ºè‹¥å¹²ä¸ªå†…å®¹åˆ—è¡¨é¡µé¢ listContent.jsp?course_id=...&content_id=...

    è¿”å› list[str]ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª URLã€‚
    """
    launcher_url = course["launcher_url"]
    course_id = course["course_id"]
    content_pages = set()

    try:
        resp = session.get(launcher_url, allow_redirects=True)
        resp.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"è®¿é—®è¯¾ç¨‹ä¸»é¡µå¤±è´¥ {launcher_url}: {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")

    for a in soup.find_all("a", href=True):
        href = a["href"]
        # å…¸å‹çš„ï¼š/webapps/blackboard/content/listContent.jsp?course_id=_83408_1&content_id=...
        if "listContent.jsp" in href and "course_id=" in href:
            # å¦‚æœéœ€è¦é™åˆ¶åªæŠ“æœ¬è¯¾ç¨‹ï¼Œå¯ä»¥åŠ ï¼š
            if course_id is None or f"course_id={course_id}" in href:
                full_url = urljoin(resp.url, href)
                content_pages.add(full_url)

    print(f"  è¯¾ç¨‹ {course_id} - {course['name']} å‘ç° {len(content_pages)} ä¸ª listContent.jsp å†…å®¹é¡µã€‚")
    return list(content_pages)

def get_first_course_name(session):
    """
    è°ƒç”¨ get_all_courses(session)ï¼Œè¿”å›ç¬¬ä¸€é—¨è¯¾ç¨‹çš„åç§°å­—ç¬¦ä¸²ã€‚
    å¦‚æœå½“å‰è´¦å·æ²¡æœ‰è¯¾ç¨‹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ""ã€‚
    """
    courses = get_all_courses(session)

    # courses æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢å¦‚ï¼š
    # { "name": "...", "course_id": "...", "launcher_url": "..." }
    first_course = courses[0]
    return first_course.get("name", "")


def start_spidering():
    """ä¸»å‡½æ•°ï¼šè®¿é—®é¡µé¢ â†’ æ”¶é›†æ–‡ä»¶é“¾æ¥ â†’ ä¸‹è½½å‰10ä¸ª"""
    downloaded_files = []

    s = login.pku_login_and_get_session(login.PKU_USERNAME, login.PKU_PASSWORD, login.COURSE_BASE_URL)

    if s is None:
        print(" ç™»å½•å¤±è´¥ï¼Œçˆ¬è™«ç»ˆæ­¢")
        return downloaded_files

    s.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0 Safari/537.36'
    })

    # course_pages = [
    #     "https://course.pku.edu.cn/webapps/portal/execute/tabs/tabAction?tab_tab_group_id=_1_1",
    #     "https://course.pku.edu.cn/webapps/blackboard/content/listContent.jsp?course_id=_86236_1&content_id=_1421420_1",
    #     "https://course.pku.edu.cn/webapps/blackboard/content/listContent.jsp?course_id=_86236_1&content_id=_1421419_1&mode=reset",
    # ]

    # 1) å…ˆè§£æå½“å‰è´¦å·çš„æ‰€æœ‰è¯¾ç¨‹ï¼ˆåå­— + course_id + launcher_urlï¼‰
    courses = get_all_courses(s)
    course_name = courses[0]['name'] if courses else 'N/A'
    print(courses) # è°ƒè¯•ä½¿ç”¨
    if not courses:
        print("æ²¡æœ‰è§£æåˆ°ä»»ä½•è¯¾ç¨‹ï¼Œçˆ¬è™«ç»ˆæ­¢ã€‚")
        return downloaded_files

    # 2) å¯¹æ¯é—¨è¯¾ç¨‹æ‰¾ listContent.jsp å†…å®¹é¡µï¼Œæ±‡æ€»æˆ course_pages
    course_pages = []
    for course in courses:
        pages = get_course_content_pages(s, course)
        course_pages.extend(pages)

    # å»é‡ä¸€ä¸‹
    course_pages = list(dict.fromkeys(course_pages))
    print(f"æ€»å…±æ”¶é›†åˆ° {len(course_pages)} ä¸ªè¯¾ç¨‹å†…å®¹é¡µï¼Œç”¨äºåç»­æ–‡ä»¶æ‰«æã€‚")


    all_file_links = []
    allowed_exts = ('.pdf', '.docx', '.pptx')

    # ç¬¬ä¸€å±‚ï¼šè®¿é—®è¯¾ç¨‹é¡µé¢ï¼Œæå–â€œè¯¦æƒ…é¡µâ€é“¾æ¥
    for page_url in course_pages:
        print(f"\n æ­£åœ¨è®¿é—®è¯¾ç¨‹é¡µé¢: {page_url}")
        try:
            resp = s.get(page_url)
            resp.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f" è®¿é—®å¤±è´¥: {e}")
            continue

        soup = BeautifulSoup(resp.text, 'html.parser')
        mid_links = []

        for a in soup.find_all('a', href=True):
            href = a['href']
            if 'content' in href and 'listContent.jsp' not in href:
                full_url = urljoin(page_url, href)
                mid_links.append(full_url)

        print(f"  å‘ç° {len(mid_links)} ä¸ªå¯èƒ½çš„æ–‡ä»¶è¯¦æƒ…é¡µ")

        # ç¬¬äºŒå±‚ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œæå–çœŸå®æ–‡ä»¶ç›´é“¾
        for mid_url in mid_links:
            if len(all_file_links) >= 3:  #  é™åˆ¶æœ€å¤šæ”¶é›†3ä¸ª
                break
            try:
                sub_resp = s.get(mid_url, stream=True)
                sub_resp.raise_for_status()
                content_type = sub_resp.headers.get("Content-Type", "").lower()

                # è‹¥ç›´æ¥è¿”å›æ–‡ä»¶ï¼Œåˆ™åŠ å…¥ä¸‹è½½é˜Ÿåˆ—
                if any(ft in content_type for ft in ["pdf", "officedocument", "ms-powerpoint", "msword"]):
                    if mid_url not in all_file_links:
                        all_file_links.append(mid_url)
                        print(f"  æ£€æµ‹åˆ°ç›´æ¥æ–‡ä»¶: {mid_url}")
                    continue

                # å¦åˆ™ç»§ç»­è§£æ HTML
                sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')
                for a2 in sub_soup.find_all('a', href=True):
                    href2 = a2['href']
                    if 'bbcswebdav' in href2 and href2.lower().endswith(allowed_exts):
                        full_url = urljoin(mid_url, href2)
                        if full_url not in all_file_links:
                            all_file_links.append(full_url)
                            print(f" æ‰¾åˆ°æ–‡ä»¶: {full_url}")
                    if len(all_file_links) >= 10:
                        break
                time.sleep(0.5)

            except Exception as e:
                print(f"  è®¿é—® {mid_url} æ—¶å‡ºé”™: {e}")

    print(f"\nğŸ” å…±å‘ç° {len(all_file_links)} ä¸ªæ–‡ä»¶ï¼ˆé™åˆ¶3ä¸ªï¼‰")

    # ä¸‹è½½é˜¶æ®µ
    for i, file_url in enumerate(all_file_links, start=1):
        print(f"\n [{i}/{len(all_file_links)}] æ­£åœ¨ä¸‹è½½: {file_url}")
        saved_path = download_file(file_url, s)
        if saved_path:
            downloaded_files.append(saved_path)
        time.sleep(1)

    print(f"ä¸‹è½½ä»»åŠ¡å®Œæˆï¼Œå…±ä¸‹è½½ {len(downloaded_files)} ä¸ªæ–‡ä»¶ã€‚")
    return downloaded_files


if __name__ == "__main__":
    start_spidering()