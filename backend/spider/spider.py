import os
import re
<<<<<<< HEAD
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from spider import login
=======
import time
>>>>>>> 2ae7a216e2d50b653a54d5d99148d85f564484c9
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

from spider import login


def get_filename_from_response(response, fallback_url):
    """ä»å“åº”å¤´æˆ–URLæå–çœŸå®æ–‡ä»¶å"""
    cd = response.headers.get("Content-Disposition", "")
    filename = None

    if "filename=" in cd:
        import re

        match = re.search(r'filename\*?=(?:UTF-8\'\')?"?([^";]+)"?', cd)
        if match:
            filename = match.group(1)
            filename = requests.utils.unquote(filename)  # è§£ç ä¸­æ–‡å

    if not filename:
        filename = os.path.basename(urlparse(fallback_url).path)

    # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•æ ¹æ® Content-Type æ¨æ–­
    if not os.path.splitext(filename)[1]:
        content_type = response.headers.get("Content-Type", "").lower()
        if "pdf" in content_type:
            filename += ".pdf"
        elif "word" in content_type:
            filename += ".docx"
        elif "powerpoint" in content_type:
            filename += ".pptx"
        else:
            filename += ".bin"

    return filename


def ensure_unique_filename(filename: str, directory: Path) -> str:
    """
    è‹¥é‡ååˆ™è‡ªåŠ¨åŠ ç¼–å·
    directory: è¦æ£€æŸ¥é‡åçš„ç›®æ ‡ç›®å½•ï¼ˆPath å¯¹è±¡ï¼‰ã€‚
    """
    base, ext = os.path.splitext(filename)
    counter = 1
    candidate = filename
    while (directory / candidate).exists():
        candidate = f"{base}_{counter}{ext}"
        counter += 1
    return candidate


def download_file(file_url, session, save_dir: str | Path | None = None):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ°æŒ‡å®šç›®å½•ï¼ˆé»˜è®¤ DOWNLOAD_DIRï¼‰ã€‚

    å‚æ•°:
        file_url:  æ–‡ä»¶çš„å®Œæ•´ URL
        session:   å·²ç™»å½•çš„ requests.Session
        save_dir:  ç›®æ ‡ä¿å­˜ç›®å½•ï¼Œå¯ä»¥æ˜¯ str æˆ– Pathï¼Œä¸ä¼ åˆ™ç”¨å…¨å±€ DOWNLOAD_DIRã€‚

    è¿”å›:
        æœ¬åœ°ä¿å­˜è·¯å¾„ï¼ˆstrï¼‰ï¼Œå¤±è´¥åˆ™è¿”å› None
    """
    try:
        resp = session.get(file_url, stream=True, timeout=60)
        resp.raise_for_status()
    except Exception as e:
        print(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {file_url}: {e}")
        return None

    # å¤„ç†ä¿å­˜ç›®å½•
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # è·å–æ–‡ä»¶åå¹¶ç¡®ä¿ä¸é‡å
    filename = get_filename_from_response(resp, file_url)
    filename = ensure_unique_filename(filename, save_dir)
    save_path = save_dir / filename

    try:
        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
    except Exception as e:
        print(f"å†™æ–‡ä»¶å¤±è´¥ {save_path}: {e}")
        return None

    print(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {save_path}")
    return str(save_path)


def _extract_pure_course_name(full_text: str) -> str:
    """
    ä»å®Œæ•´è¯¾ç¨‹æ ‡é¢˜ä¸­æå–â€œçº¯å‡€è¯¾ç¨‹åâ€ã€‚

    ä¾‹å¦‚ï¼š
        "25261-00011-...: äººç±»çš„æ€§ã€ç”Ÿè‚²ä¸å¥åº·(25-26å­¦å¹´ç¬¬1å­¦æœŸ)"
    ->  "äººç±»çš„æ€§ã€ç”Ÿè‚²ä¸å¥åº·"
    """
    # 1) å»æ‰å‰é¢çš„è¯¾å·éƒ¨åˆ†ï¼ˆä»¥ç¬¬ä¸€ä¸ªå†’å· : ä¸ºç•Œï¼‰
    parts = full_text.split(":", 1)
    after_code = parts[1].strip() if len(parts) == 2 else full_text.strip()

    # 2) å»æ‰æœ«å°¾çš„å­¦æœŸæè¿°æ‹¬å· "(25-26å­¦å¹´ç¬¬1å­¦æœŸ)" / "(25-26å­¦å¹´ç¬¬1å­¦æœŸæœ¬ç ”åˆä¸Š)" ç­‰
    paren_idx = after_code.rfind("(")
    if paren_idx != -1:
        pure_name = after_code[:paren_idx].strip()
    else:
        pure_name = after_code.strip()

    return pure_name


def get_current_semester_course_list(session):
    """
    ä½¿ç”¨ä¼ å…¥çš„å·²ç™»å½• Sessionï¼Œä»é¦–é¡µâ€œå½“å‰å­¦æœŸè¯¾ç¨‹â€åŒºå—æŠ“å–è¯¾ç¨‹åˆ—è¡¨ã€‚
    å‚æ•°ï¼š
        session: å·²ç™»å½•çš„ requests.Sessionï¼Œå¯¹åº” pku_login_and_get_session è¿”å›å€¼
    è¿”å›ï¼š
        ä¸€ä¸ª Python åˆ—è¡¨ï¼ˆå¯ç›´æ¥ç”¨äº jsonifyï¼‰ï¼š
        [
            {"id": 1, "name": "äººç±»çš„æ€§ã€ç”Ÿè‚²ä¸å¥åº·"},
            {"id": 2, "name": "å¤ªææ‹³"},
            ...
        ]
    """
    if session is None:
        print("ä¼ å…¥çš„ Session ä¸º Noneï¼Œæ— æ³•è·å–è¯¾ç¨‹åˆ—è¡¨")
        return []

    # 1. è®¿é—®å¯èƒ½åŒ…å«â€œå½“å‰å­¦æœŸè¯¾ç¨‹æ¡†â€çš„é¡µé¢
    candidate_urls = [
        login.COURSE_BASE_URL,
        "https://course.pku.edu.cn/webapps/portal/execute/tabs/tabAction?tab_tab_group_id=_1_1",
    ]

    found_ul = None

    for url in candidate_urls:
        try:
            resp = session.get(url, allow_redirects=True, timeout=15)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            # ç²¾å‡†æ‰¾ <ul class="portletList-img courseListing coursefakeclass ">
            # æ³¨æ„ class æœ‰å¤šä¸ªï¼Œæ‰€ä»¥ç”¨ lambda åˆ¤æ–­åŒ…å« 'coursefakeclass'
            ul = soup.find(
                "ul", attrs={"class": lambda v: v and "coursefakeclass" in v}
            )
            if ul:
                found_ul = ul
                print(f"åœ¨é¡µé¢ {resp.url} æ‰¾åˆ°å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨åŒºå—ã€‚")
                break
        except requests.exceptions.RequestException as e:
            print(f"è®¿é—® {url} å¤±è´¥: {e}")
            continue

    if not found_ul:
        print("æ²¡æœ‰æ‰¾åˆ°å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨çš„ <ul class='... coursefakeclass ...'> åŒºå—ã€‚")
        return []

    # 2. è§£æè¯¥ <ul> ä¸­æ‰€æœ‰ <a>ï¼Œå¹¶æŠŠè¯¾ç¨‹åæ¸…æ´—æˆâ€œçº¯å‡€åç§°â€
    result = []

    for idx, a in enumerate(found_ul.find_all("a", href=True), start=1):
        full_text = a.get_text(strip=True)
        pure_name = _extract_pure_course_name(full_text)

        result.append(
            {
                "id": idx,
                "name": pure_name,
            }
        )

    print(f"å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨å…± {len(result)} é—¨ï¼š")
    for c in result:
        print(f"  - {c['id']}: {c['name']}")

    return result


def download_handouts_for_course(
    session,
    course_id,
    section_names=None,
    max_files=3,
    download_root: str | Path | None = None,
):
    """
    æ ¹æ®å‰ç«¯é€‰ä¸­çš„è¯¾ç¨‹ idï¼ˆ1 å¼€å§‹ï¼Œå¯¹åº”â€œå½“å‰å­¦æœŸè¯¾ç¨‹â€åˆ—è¡¨é‡Œçš„ç¬¬å‡ ä¸ª <li>ï¼‰ï¼Œ
    è¿›å…¥è¯¥è¯¾ç¨‹åï¼Œå¯»æ‰¾å·¦ä¾§è¾¹æ ä¸­åç§°ä¸º â€œè¯¾ç¨‹è®²ä¹‰â€ / â€œè¯¾ç¨‹æ–‡ä»¶â€ ç­‰æ ç›®çš„é¡µé¢ï¼Œ
    ä¸‹è½½å…¶ä¸­æ‰€æœ‰ /bbcswebdav/... å½¢å¼çš„æ–‡ä»¶åˆ° downloads æ–‡ä»¶å¤¹ã€‚

    å‚æ•°:
        session: å·²ç™»å½•çš„ requests.Sessionï¼ˆå¤–éƒ¨è´Ÿè´£ç™»å½•å¥½å¹¶ä¼ è¿›æ¥ï¼‰
        course_id: 1 å¼€å§‹çš„æ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼ˆ"1"ã€"2"...ï¼‰
        section_names: è¦éå†çš„æ ç›®ååˆ—è¡¨ï¼Œé»˜è®¤ ["è¯¾ç¨‹è®²ä¹‰"]
        max_files: æœ€å¤šä¸‹è½½å¤šå°‘ä¸ªæ–‡ä»¶
        download_root: ä¸‹è½½æ ¹ç›®å½•

    è¿”å›:
        downloaded_files: List[dict]ï¼Œæ¯ä¸ªå…ƒç´ å½¢å¦‚ï¼š
            {
                "path": "æœ¬åœ°ä¿å­˜è·¯å¾„",
                "name": "å¯è§æ–‡ä»¶åï¼ˆä¾‹å¦‚ 2025ç§‹å­£è½¯ä»¶å·¥ç¨‹è¯¾ç¨‹ä»‹ç»20250909ï¼‰"
            }
    """
    if session is None:
        print("ä¼ å…¥çš„ Session ä¸º Noneï¼Œçˆ¬è™«ç»ˆæ­¢")
        return []

    # 0. course_id å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè½¬æˆ int
    try:
        idx = int(course_id)
    except (TypeError, ValueError):
        print(f"æ— æ•ˆçš„è¯¾ç¨‹ id: {course_id}")
        return []

    if section_names is None:
        section_names = ["è¯¾ç¨‹è®²ä¹‰"]

    downloaded_files: list[dict[str, str]] = []

    # 1. å…ˆæ‰¾åˆ°â€œå½“å‰å­¦æœŸè¯¾ç¨‹â€ <ul class="... coursefakeclass ...">
    candidate_urls = [
        login.COURSE_BASE_URL,
        "https://course.pku.edu.cn/webapps/portal/execute/tabs/tabAction?tab_tab_group_id=_1_1",
    ]

    found_ul = None
    base_url = None

    for url in candidate_urls:
        try:
            resp = session.get(url, allow_redirects=True, timeout=15)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            ul = soup.find(
                "ul", attrs={"class": lambda v: v and "coursefakeclass" in v}
            )
            if ul:
                found_ul = ul
                base_url = resp.url
                print(f"åœ¨é¡µé¢ {resp.url} æ‰¾åˆ°å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨åŒºå—ã€‚")
                break
        except requests.exceptions.RequestException as e:
            print(f"è®¿é—® {url} å¤±è´¥: {e}")
            continue

    if not found_ul:
        print("æ²¡æœ‰æ‰¾åˆ°å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨çš„ <ul class='... coursefakeclass ...'> åŒºå—ã€‚")
        return downloaded_files

    # 2. å–è¿™ä¸ª <ul> é‡Œæ‰€æœ‰è¯¾ç¨‹ <a>ï¼ŒæŒ‰é¡ºåºç¼–å·ï¼Œä» 1 å¼€å§‹
    course_links = [a for a in found_ul.find_all("a", href=True)]
    if not course_links:
        print("å½“å‰å­¦æœŸè¯¾ç¨‹åˆ—è¡¨ä¸­æ²¡æœ‰ä»»ä½•é“¾æ¥ã€‚")
        return downloaded_files

    if idx < 1 or idx > len(course_links):
        print(f"è¯¾ç¨‹ id è¶Šç•Œ: {idx}, å½“å‰å­¦æœŸå…±æœ‰ {len(course_links)} é—¨è¯¾")
        return downloaded_files

    target_a = course_links[idx - 1]
    full_text = target_a.get_text(strip=True)
    pure_name = _extract_pure_course_name(full_text)
    launcher_url = urljoin(base_url, target_a["href"]) if base_url else target_a["href"]

    print(f"é€‰æ‹©ç¬¬ {idx} é—¨è¯¾ç¨‹ï¼š{pure_name}")
    print(f"è¯¾ç¨‹å…¥å£é“¾æ¥ï¼š{launcher_url}")

    # 3. æ‰“å¼€è¿™é—¨è¯¾çš„ä¸»é¡µ
    try:
        course_resp = session.get(launcher_url, allow_redirects=True)
        course_resp.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"è®¿é—®è¯¾ç¨‹ä¸»é¡µå¤±è´¥ {launcher_url}: {e}")
        return downloaded_files

    course_soup = BeautifulSoup(course_resp.text, "html.parser")

    # 4. åœ¨å·¦ä¾§è¾¹æ å¯»æ‰¾åç§°ä¸º section_names ä¸­ä»»æ„ä¸€ä¸ªçš„æ ç›®é“¾æ¥
    section_urls = []
    target_set = set(section_names)

    for a in course_soup.find_all("a", href=True):
        span = a.find("span")
        title = span.get("title") if span else None
        text = span.get_text(strip=True) if span else a.get_text(strip=True)

        if title in target_set or text in target_set:
            sec_url = urljoin(course_resp.url, a["href"])
            if sec_url not in section_urls:
                section_urls.append(sec_url)

    if not section_urls:
        print(f"æ²¡æœ‰åœ¨è¯¾ç¨‹ä¸»é¡µå·¦ä¾§æ æ‰¾åˆ°æŒ‡å®šæ ç›®ï¼š{section_names}")
        return downloaded_files

    print(f"æ‰¾åˆ° {len(section_urls)} ä¸ªæ ç›®é“¾æ¥ï¼š")
    for u in section_urls:
        print("  -", u)

    # 5. ä¾æ¬¡è¿›å…¥æ¯ä¸ªæ ç›®é¡µé¢ï¼ŒæŸ¥æ‰¾ /bbcswebdav/... æ–‡ä»¶é“¾æ¥ + å¯è§æ–‡ä»¶å
    all_file_links: list[dict[str, str]] = []

    for sec_url in section_urls:
        print(f"\nè¿›å…¥æ ç›®é¡µé¢: {sec_url}")
        try:
            sec_resp = session.get(sec_url)
            sec_resp.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"è®¿é—®æ ç›®é¡µé¢å¤±è´¥ {sec_url}: {e}")
            continue

        sec_soup = BeautifulSoup(sec_resp.text, "html.parser")

        for a in sec_soup.find_all("a", href=True):
            href = a["href"]
            if "bbcswebdav" in href:
                full_url = urljoin(sec_resp.url, href)

                # æå–å¯è§æ–‡ä»¶åï¼Œä¾‹å¦‚ "æ–‡ä»¶2025ç§‹å­£è½¯ä»¶å·¥ç¨‹è¯¾ç¨‹ä»‹ç»20250909"
                raw_text = a.get_text(strip=True)
                display_name = raw_text

                # å¸¸è§æƒ…å†µï¼šå‰é¢æœ‰ä¸€ä¸ª "æ–‡ä»¶"ï¼ˆæ¥è‡ª <img alt="æ–‡ä»¶">ï¼‰
                if display_name.startswith("æ–‡ä»¶"):
                    display_name = display_name[len("æ–‡ä»¶") :].strip()

                # å»é‡ï¼šæŒ‰ URL å»é‡
                if not any(item["url"] == full_url for item in all_file_links):
                    all_file_links.append(
                        {
                            "url": full_url,
                            "name": display_name,
                        }
                    )
                    print(f"  æ‰¾åˆ°æ–‡ä»¶é“¾æ¥: {full_url}  åç§°: {display_name}")

    print(f"\nğŸ” åœ¨æ ç›® {section_names} ä¸­å…±å‘ç° {len(all_file_links)} ä¸ªæ–‡ä»¶é“¾æ¥ã€‚")

    # 6. é€ä¸ªä¸‹è½½è¿™äº›æ–‡ä»¶ï¼Œå¹¶æ§åˆ¶æ•°é‡
    for i, file_info in enumerate(all_file_links, start=1):
        if max_files is not None and i > max_files:
            print(f"\nâš ï¸ å·²è¾¾åˆ° max_files={max_files}ï¼Œåœæ­¢ç»§ç»­ä¸‹è½½ã€‚")
            break

        file_url = file_info["url"]
        display_name = file_info["name"]

        print(f"\n [{i}/{len(all_file_links)}] æ­£åœ¨ä¸‹è½½: {file_url} ({display_name})")
        saved_path = download_file(file_url, session, save_dir=download_root)
        if saved_path is not None:
            downloaded_files.append(
                {
                    "path": saved_path,
                    "name": display_name,
                }
            )
        time.sleep(1)

    print(f"\nâœ… æ ç›®æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œå…±ä¸‹è½½ {len(downloaded_files)} ä¸ªæ–‡ä»¶ã€‚")
    return downloaded_files
