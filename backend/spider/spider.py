import time
import requests
import os
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from . import login

DOWNLOAD_DIR = 'downloads'
if not os.path.exists(DOWNLOAD_DIR):
    os.makedirs(DOWNLOAD_DIR)

def get_filename_from_response(response, fallback_url):
    
    """ä»å“åº”å¤´æˆ–URLæå–çœŸå®æ–‡ä»¶å"""
    cd = response.headers.get('Content-Disposition', '')
    filename = None

    if 'filename=' in cd:
        import re
        match = re.search(r'filename\*?=(?:UTF-8\'\')?"?([^";]+)"?', cd)
        if match:
            filename = match.group(1)
            filename = requests.utils.unquote(filename)  # è§£ç ä¸­æ–‡å

    if not filename:
        filename = os.path.basename(urlparse(fallback_url).path)

    # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•æ ¹æ® Content-Type æ¨æ–­
    if not os.path.splitext(filename)[1]:
        content_type = response.headers.get("Content-Type", "").lower()
        if "pdf" in content_type:
            filename += ".pdf"
        elif "word" in content_type:
            filename += ".docx"
        elif "powerpoint" in content_type:
            filename += ".pptx"
        else:
            filename += ".bin"

    return filename


def ensure_unique_filename(filename):
    """è‹¥é‡ååˆ™è‡ªåŠ¨åŠ ç¼–å·"""
    base, ext = os.path.splitext(filename)
    counter = 1
    while os.path.exists(os.path.join(DOWNLOAD_DIR, filename)):
        filename = f"{base}_{counter}{ext}"
        counter += 1
    return filename


def download_file(file_url, session):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ° downloads æ–‡ä»¶å¤¹"""
    try:
        response = session.get(file_url, stream=True)
        response.raise_for_status()

        filename = get_filename_from_response(response, file_url)
        filename = ensure_unique_filename(filename)
        save_path = os.path.join(DOWNLOAD_DIR, filename)

        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {save_path}")
        return save_path
    except Exception as e:
        print(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {file_url}: {e}")
        return None


def start_spidering():
    """ä¸»å‡½æ•°ï¼šè®¿é—®é¡µé¢ â†’ æ”¶é›†æ–‡ä»¶é“¾æ¥ â†’ ä¸‹è½½å‰10ä¸ª"""
    downloaded_files = []

    s = login.pku_login_and_get_session(login.PKU_USERNAME, login.PKU_PASSWORD, login.COURSE_BASE_URL)
    if s is None:
        print(" ç™»å½•å¤±è´¥ï¼Œçˆ¬è™«ç»ˆæ­¢")
        return downloaded_files

    s.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0 Safari/537.36'
    })

    course_pages = [
        "https://course.pku.edu.cn/webapps/portal/execute/tabs/tabAction?tab_tab_group_id=_1_1",
        "https://course.pku.edu.cn/webapps/blackboard/content/listContent.jsp?course_id=_86236_1&content_id=_1421420_1",
        "https://course.pku.edu.cn/webapps/blackboard/content/listContent.jsp?course_id=_86236_1&content_id=_1421419_1&mode=reset",
    ]

    all_file_links = []
    allowed_exts = ('.pdf', '.docx', '.pptx')

    # ç¬¬ä¸€å±‚ï¼šè®¿é—®è¯¾ç¨‹é¡µé¢ï¼Œæå–â€œè¯¦æƒ…é¡µâ€é“¾æ¥
    for page_url in course_pages:
        print(f"\n æ­£åœ¨è®¿é—®è¯¾ç¨‹é¡µé¢: {page_url}")
        try:
            resp = s.get(page_url)
            resp.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f" è®¿é—®å¤±è´¥: {e}")
            continue

        soup = BeautifulSoup(resp.text, 'html.parser')
        mid_links = []

        for a in soup.find_all('a', href=True):
            href = a['href']
            if 'content' in href and 'listContent.jsp' not in href:
                full_url = urljoin(page_url, href)
                mid_links.append(full_url)

        print(f"  å‘ç° {len(mid_links)} ä¸ªå¯èƒ½çš„æ–‡ä»¶è¯¦æƒ…é¡µ")

        # ç¬¬äºŒå±‚ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œæå–çœŸå®æ–‡ä»¶ç›´é“¾
        for mid_url in mid_links:
            if len(all_file_links) >= 10:  #  é™åˆ¶æœ€å¤šæ”¶é›†10ä¸ª
                break
            try:
                sub_resp = s.get(mid_url, stream=True)
                sub_resp.raise_for_status()
                content_type = sub_resp.headers.get("Content-Type", "").lower()

                # è‹¥ç›´æ¥è¿”å›æ–‡ä»¶ï¼Œåˆ™åŠ å…¥ä¸‹è½½é˜Ÿåˆ—
                if any(ft in content_type for ft in ["pdf", "officedocument", "ms-powerpoint", "msword"]):
                    if mid_url not in all_file_links:
                        all_file_links.append(mid_url)
                        print(f"  æ£€æµ‹åˆ°ç›´æ¥æ–‡ä»¶: {mid_url}")
                    continue

                # å¦åˆ™ç»§ç»­è§£æ HTML
                sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')
                for a2 in sub_soup.find_all('a', href=True):
                    href2 = a2['href']
                    if 'bbcswebdav' in href2 and href2.lower().endswith(allowed_exts):
                        full_url = urljoin(mid_url, href2)
                        if full_url not in all_file_links:
                            all_file_links.append(full_url)
                            print(f" æ‰¾åˆ°æ–‡ä»¶: {full_url}")
                    if len(all_file_links) >= 10:
                        break
                time.sleep(0.5)

            except Exception as e:
                print(f"  è®¿é—® {mid_url} æ—¶å‡ºé”™: {e}")

    print(f"\nğŸ” å…±å‘ç° {len(all_file_links)} ä¸ªæ–‡ä»¶ï¼ˆé™åˆ¶10ä¸ªï¼‰")

    # ä¸‹è½½é˜¶æ®µ
    for i, file_url in enumerate(all_file_links, start=1):
        print(f"\n [{i}/{len(all_file_links)}] æ­£åœ¨ä¸‹è½½: {file_url}")
        saved_path = download_file(file_url, s)
        if saved_path:
            downloaded_files.append(saved_path)
        time.sleep(1)

    print(f"\ ä¸‹è½½ä»»åŠ¡å®Œæˆï¼Œå…±ä¸‹è½½ {len(downloaded_files)} ä¸ªæ–‡ä»¶ã€‚")
    return downloaded_files


if __name__ == "__main__":
    start_spidering()
